<?xml version="1.0" encoding="utf-8"?>
<nns>
  <nn name="VGGD_NoBatchNorm" cat="VGG">
    <modules>
      <!-- Block#1 -->
      <module name="C1"  type="conv2d" in_channels="3"   out_channels="64"  kernel_size="3" padding="1" />
      <module name="C3"  type="conv2d" in_channels="64"  out_channels="64"  kernel_size="3" padding="1" />
      <!-- Block#2 -->
      <module name="C6"  type="conv2d" in_channels="64"  out_channels="128" kernel_size="3" padding="1" />
      <module name="C8"  type="conv2d" in_channels="128" out_channels="128" kernel_size="3" padding="1" />
      <!-- Block#3 -->
      <module name="C11" type="conv2d" in_channels="128" out_channels="256" kernel_size="3" padding="1" />
      <module name="C13" type="conv2d" in_channels="256" out_channels="256" kernel_size="3" padding="1" />
      <module name="C15" type="conv2d" in_channels="256" out_channels="256" kernel_size="3" padding="1" />
      <!-- Block#4 -->
      <module name="C18" type="conv2d" in_channels="256" out_channels="512" kernel_size="3" padding="1" />
      <module name="C20" type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <module name="C22" type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <!-- Block#5 -->
      <module name="C25" type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <module name="C27" type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <module name="C29" type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <!-- FC -->
      <module name="FC32" type="linear" in_features="25088" out_features="4096" />
      <module name="FC35" type="linear" in_features="4096"  out_features="4096" />
      <module name="FC38" type="linear" in_features="4096"  out_features="1000" />
    </modules>
    <forward>
      <f module="C1" />
      <f functional="relu" inplace="true" />
      <f module="C3" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f module="C6" />
      <f functional="relu" inplace="true" />
      <f module="C8" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f module="C11" />
      <f functional="relu" inplace="true" />
      <f module="C13" />
      <f functional="relu" inplace="true" />
      <f module="C15" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f module="C18" />
      <f functional="relu" inplace="true" />
      <f module="C20" />
      <f functional="relu" inplace="true" />
      <f module="C22" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f module="C25" />
      <f functional="relu" inplace="true" />
      <f module="C27" />
      <f functional="relu" inplace="true" />
      <f module="C29" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f view="flat" />
      <f module="FC32" />
      <f functional="relu" inplace="true" />
      <!-- drop-out can't set 'inplace' to true, otherwise, the backward will fail. -->
      <f functional="dropout" inplace="false" p="0.5" />
      <f module="FC35" />
      <f functional="relu" inplace="true" />
      <!-- drop-out can't set 'inplace' to true, otherwise, the backward will fail. -->
      <f functional="dropout" inplace="false" p="0.5" />
      <f module="FC38" />
    </forward>
  </nn>
  <nn name="VGGD_BatchNorm" cat="VGG">
    <modules>
      <!-- Block#1 -->
      <module name="C1"   type="conv2d" in_channels="3"   out_channels="64"  kernel_size="3" padding="1" />
      <module name="C1B"  type="batchnorm2d" num_features="64" />
      <module name="C3"   type="conv2d" in_channels="64"  out_channels="64"  kernel_size="3" padding="1" />
      <module name="C3B"  type="batchnorm2d" num_features="64" />
      <!-- Block#2 -->
      <module name="C6"   type="conv2d" in_channels="64"  out_channels="128" kernel_size="3" padding="1" />
      <module name="C6B"  type="batchnorm2d" num_features="128" />
      <module name="C8"   type="conv2d" in_channels="128" out_channels="128" kernel_size="3" padding="1" />
      <module name="C8B"  type="batchnorm2d" num_features="128" />
      <!-- Block#3 -->
      <module name="C11"  type="conv2d" in_channels="128" out_channels="256" kernel_size="3" padding="1" />
      <module name="C11B" type="batchnorm2d" num_features="256" />
      <module name="C13"  type="conv2d" in_channels="256" out_channels="256" kernel_size="3" padding="1" />
      <module name="C13B" type="batchnorm2d" num_features="256" />
      <module name="C15"  type="conv2d" in_channels="256" out_channels="256" kernel_size="3" padding="1" />
      <module name="C15B" type="batchnorm2d" num_features="256" />
      <!-- Block#4 -->
      <module name="C18"  type="conv2d" in_channels="256" out_channels="512" kernel_size="3" padding="1" />
      <module name="C18B" type="batchnorm2d" num_features="512" />
      <module name="C20"  type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <module name="C20B" type="batchnorm2d" num_features="512" />
      <module name="C22"  type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <module name="C22B" type="batchnorm2d" num_features="512" />
      <!-- Block#5 -->
      <module name="C25"  type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <module name="C25B" type="batchnorm2d" num_features="512" />
      <module name="C27"  type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <module name="C27B" type="batchnorm2d" num_features="512" />
      <module name="C29"  type="conv2d" in_channels="512" out_channels="512" kernel_size="3" padding="1" />
      <module name="C29B" type="batchnorm2d" num_features="512" />
      <!-- FC -->
      <module name="FC32" type="linear" in_features="25088" out_features="4096" />
      <module name="FC35" type="linear" in_features="4096"  out_features="4096" />
      <module name="FC38" type="linear" in_features="4096"  out_features="1000" />
    </modules>
    <forward>
      <f module="C1" />
      <f module="C1B" />
      <f functional="relu" inplace="true" />
      <f module="C3" />
      <f module="C3B" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f module="C6" />
      <f module="C6B" />
      <f functional="relu" inplace="true" />
      <f module="C8" />
      <f module="C8B" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f module="C11" />
      <f module="C11B" />
      <f functional="relu" inplace="true" />
      <f module="C13" />
      <f module="C13B" />
      <f functional="relu" inplace="true" />
      <f module="C15" />
      <f module="C15B" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f module="C18" />
      <f module="C18B" />
      <f functional="relu" inplace="true" />
      <f module="C20" />
      <f module="C20B" />
      <f functional="relu" inplace="true" />
      <f module="C22" />
      <f module="C22B" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f module="C25" />
      <f module="C25B" />
      <f functional="relu" inplace="true" />
      <f module="C27" />
      <f module="C27B" />
      <f functional="relu" inplace="true" />
      <f module="C29" />
      <f module="C29B" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="2" />
      <f view="flat" />
      <f module="FC32" />
      <f functional="relu" inplace="true" />
      <!-- drop-out can't set 'inplace' to true, otherwise, the backward will fail. -->
      <f functional="dropout" inplace="false" p="0.5" />
      <f module="FC35" />
      <f functional="relu" inplace="true" />
      <!-- drop-out can't set 'inplace' to true, otherwise, the backward will fail. -->
      <f functional="dropout" inplace="false" p="0.5" />
      <f module="FC38" />
    </forward>
  </nn>
  <nn name="ResNet18" cat="ResNet">
    <modules>
      <module name="conv1"      type="conv2d" in_channels="3" out_channels="64" kernel_size="3" padding="3" stride="2" bias="false" />
      <module name="bn1"        type="batchnorm2d" num_features="64" />
      <!-- layer#1 -->
      <!-- block#1 -->
      <module name="conv_1_1_1" type="conv2d" in_channels="64" out_channels="64" kernel_size="3" padding="1" bias="0"/>
      <module name="btn_1_1_1"  type="batchnorm2d" num_features="64" />
      <module name="conv_1_1_2" type="conv2d" in_channels="64" out_channels="64" kernel_size="3" bias="0" />
      <module name="btn_1_1_2"  type="batchnorm2d" num_features="64" />
      <!-- block#2-->
      <module name="conv_1_2_1" type="conv2d" in_channels="64" out_channels="64" kernel_size="3" padding="1" bias="0" />
      <module name="btn_1_2_1"  type="batchnorm2d" num_features="64" />
      <module name="conv_1_2_2" type="conv2d" in_channels="64" out_channels="64" kernel_size="3" padding="1" bias="0" />
      <module name="btn_1_2_2"  type="batchnorm2d" num_features="64" />
      <!-- layer#2 -->
      <!-- block#1 -->
      <module name="conv_2_1_1" type="conv2d" in_channels="64" out_channels="128" kernel_size="3" stride="2" padding="1" bias="0" />
      <module name="btn_2_1_1"  type="batchnorm2d" num_features="128" />
      <module name="conv_2_1_2" type="conv2d" in_channels="128" out_channels="128" kernel_size="3" bias="0" />
      <module name="btn_2_1_2"  type="batchnorm2d" num_features="128" />
      <module name="conv_2_1_3" type="conv2d" in_channels="64" out_channels="128" kernel_size="1" stride="2" bias="0" />
      <module name="btn_2_1_3"  type="batchnorm2d" num_features="128" />
      <!-- block#2 -->
      <module name="conv_2_2_1" type="conv2d" in_channels="128" out_channels="128" kernel_size="3" bias="0" />
      <module name="btn_2_2_1"  type="batchnorm2d" num_features="128" />
      <module name="conv_2_2_2" type="conv2d" in_channels="128" out_channels="128" kernel_size="3" bias="0" />
      <module name="btn_2_2_2"  type="batchnorm2d" num_features="128" />
      <!-- layer#3 -->
      <!-- block#1 -->
      <module name="conv_3_1_1" type="conv2d" in_channels="128" out_channels="256" kernel_size="3" stride="2" padding="1" bias="0" />
      <module name="btn_3_1_1"  type="batchnorm2d" num_features="256" />
      <module name="conv_3_1_2" type="conv2d" in_channels="256" out_channels="256" kernel_size="3" bias="0" />
      <module name="btn_3_1_2"  type="batchnorm2d" num_features="256" />
      <module name="conv_3_1_3" type="conv2d" in_channels="128" out_channels="256" kernel_size="1" stride="2" bias="0" />
      <module name="btn_3_1_3"  type="batchnorm2d" num_features="256" />
      <!-- block#2 -->
      <module name="conv_3_2_1" type="conv2d" in_channels="256" out_channels="256" kernel_size="3" bias="0" />
      <module name="btn_3_2_1"  type="batchnorm2d" num_features="256" />
      <module name="conv_3_2_2" type="conv2d" in_channels="256" out_channels="256" kernel_size="3" bias="0" />
      <module name="btn_3_2_2"  type="batchnorm2d" num_features="256" />
      <!-- layer#4 -->
      <!-- block#1 -->
      <module name="conv_4_1_1" type="conv2d" in_channels="256" out_channels="512" kernel_size="3" stride="2" padding="1" bias="0" />
      <module name="btn_4_1_1"  type="batchnorm2d" num_features="512" />
      <module name="conv_4_1_2" type="conv2d" in_channels="512" out_channels="512" kernel_size="3" bias="0" />
      <module name="btn_4_1_2"  type="batchnorm2d" num_features="512" />
      <module name="conv_4_1_3" type="conv2d" in_channels="256" out_channels="512" kernel_size="1" stride="2" bias="0" />
      <module name="btn_4_1_3"  type="batchnorm2d" num_features="512" />
      <!-- block#2 -->
      <module name="conv_4_2_1" type="conv2d" in_channels="512" out_channels="512" kernel_size="3" bias="0" />
      <module name="btn_4_2_1"  type="batchnorm2d" num_features="512" />
      <module name="conv_4_2_2" type="conv2d" in_channels="512" out_channels="512" kernel_size="3" bias="0" />
      <module name="btn_4_2_2"  type="batchnorm2d" num_features="512" />
      <!-- avgpool -->
      <module name="avgpool" type="adaptiveavgpool2d" outputsize="1" />
      <!-- full connection -->
      <module name="fc" type="linear" in_features="4096"  out_features="1000" />
    </modules>
    <forward>
      <f module="conv1" />
      <f module="btn1" />
      <f functional="relu" inplace="true" />
      <f functional="max_pool2d" kernel_size="3" stride="2" padding="1" />
      <!-- layer#1 -->
      <!-- block#1 -->
      <f module="conv_1_1_1" />
      <f module="btn_1_1_1" />
      <f functional="relu" />
      <f module="conv_1_1_2" />
      <f module="btn_1_1_2" />
      <!-- block#2 -->
      <f module="conv_1_2_1" />
      <f module="btn_1_2_1" />
      <f functional="relu" />
      <f module="conv_1_2_2" />
      <f module="btn_1_2_2" />
      <!-- layer#2 -->
      <!-- block#1 -->
      <parallel mergeop="plus">
        <branch>
          <f module="conv_2_1_1" />
          <f module="btn_2_1_1" />
          <f functional="relu" />
          <f module="conv_2_1_2" />
          <f module="btn_2_1_2" />
        </branch>
        <branch>
          <f module="conv_2_1_3" />
          <f module="btn_2_1_3" />
        </branch>
      </parallel>
      <!-- block#2 -->
      <f module="conv_2_2_1" />
      <f module="btn_2_2_1" />
      <f functional="relu" />
      <f module="conv_2_2_2" />
      <f module="btn_2_2_2" />
      <!-- layer#3 -->
      <!-- block#1 -->
      <parallel mergeop="plus">
        <branch>
          <f module="conv_3_1_1" />
          <f module="btn_3_1_1" />
          <f functional="relu" />
          <f module="conv_3_1_2" />
          <f module="btn_3_1_2" />
        </branch>
        <branch>
          <f module="conv_3_1_3" />
          <f module="btn_3_1_3" />
        </branch>
      </parallel>
      <!-- block#2 -->
      <f module="conv_3_2_1" />
      <f module="btn_3_2_1" />
      <f functional="relu" />
      <f module="conv_3_2_2" />
      <f module="btn_3_2_2" />
      <!-- layer#4 -->
      <!-- block#1 -->
      <parallel mergeop="plus">
        <branch>
          <f module="conv_4_1_1" />
          <f module="btn_4_1_1" />
          <f functional="relu" />
          <f module="conv_4_1_2" />
          <f module="btn_4_1_2" />
        </branch>
        <branch>
          <f module="conv_4_1_3" />
          <f module="btn_4_1_3" />
        </branch>
      </parallel>
      <!-- block#2 -->
      <f module="conv_4_2_1" />
      <f module="btn_4_2_1" />
      <f functional="relu" />
      <f module="conv_4_2_2" />
      <f module="btn_4_2_2" />
      <!-- avgpool -->
      <f module="avgpool" />
      <f module="fc" />
    </forward>
  </nn>
</nns>
